# Importamos StandardScaler para estandarizar las variables num칠ricas
from sklearn.preprocessing import StandardScaler

# Creamos una instancia del escalador
scaler = StandardScaler()

# Lista de columnas num칠ricas que vamos a estandarizar
num_vars = ['BMI', 'Age', 'PhysHlth', 'HighBP', 'GenHlth', 'DiffWalk', 'HighChol']

# Aplicamos la estandarizaci칩n: media 0, desviaci칩n est치ndar 1
# fit_transform calcula los par치metros y transforma los datos
data_clean[num_vars] = scaler.fit_transform(data_clean[num_vars])

# Convertimos las variables categ칩ricas 'Education' e 'Income' en variables dummy (0 y 1)
# drop_first=True elimina la primera categor칤a para evitar colinealidad
data_clean = pd.get_dummies(data_clean, columns=['Education', 'Income'], drop_first=True)

# Importamos SMOTE (Synthetic Minority Over-sampling Technique) para balancear clases desbalanceadas
from imblearn.over_sampling import SMOTE

# Separamos las caracter칤sticas (X) de la variable objetivo (y)
X = data_clean.drop('Diabetes_binary', axis=1)  # Todas las columnas menos 'Diabetes_binary'
y = data_clean['Diabetes_binary']               # Variable objetivo

# Verificamos si hay un desbalance significativo en la clase
# Si m치s del 70% de los datos pertenecen a una sola clase (y[0]), aplicamos SMOTE
if y.value_counts()[0] / len(y) > 0.7:
    # Aplicamos SMOTE para generar ejemplos sint칠ticos de la clase minoritaria
    X, y = SMOTE().fit_resample(X, y)

    # Reconstruimos el DataFrame original incluyendo las nuevas muestras
    data_clean = pd.concat([X, y], axis=1)

# Para la variable BMI, eliminamos valores extremos (outliers)

# Calculamos el percentil 5 (Q1) y el percentil 95 (Q3) de la columna 'BMI'
Q1 = data_clean['BMI'].quantile(0.05)  # L칤mite inferior (5% m치s bajo)
Q3 = data_clean['BMI'].quantile(0.95)  # L칤mite superior (5% m치s alto)

# Filtramos el DataFrame conservando solo los valores dentro del rango [Q1, Q3]
# Es decir, eliminamos el 5% m치s bajo y el 5% m치s alto para reducir el efecto de los outliers
data_clean = data_clean[(data_clean['BMI'] >= Q1) & (data_clean['BMI'] <= Q3)]

# Importamos el modelo de Gradient Boosting y m칠tricas para evaluar
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Creamos el modelo de Gradient Boosting con par치metros ajustados
gb_model = GradientBoostingClassifier(
    n_estimators=150,         # N칰mero de 치rboles (estimadores)
    learning_rate=1,          # Tasa de aprendizaje (m치s alto = m치s agresivo)
    max_depth=5,              # Profundidad m치xima de cada 치rbol
    min_samples_split=20,     # M칤nimo de muestras para dividir un nodo
    min_samples_leaf=10,      # M칤nimo de muestras en una hoja
    max_features='sqrt',      # N칰mero de features consideradas al dividir (ra칤z cuadrada del total)
    subsample=0.7,            # Fracci칩n de muestras usadas en cada 치rbol (bagging)
    random_state=38           # Semilla para reproducibilidad
)

# Entrenamos el modelo con los datos de entrenamiento
gb_model.fit(X_train_B, y_train_B)

# Realizamos predicciones sobre el conjunto de prueba
y_pred_B_1 = gb_model.predict(X_test_B)

# Mostramos la precisi칩n del modelo
print(f"Accuracy: {accuracy_score(y_test_B, y_pred_B_1):.4f}")

# Mostramos la matriz de confusi칩n en forma de heatmap
print("\nMatriz de Confusi칩n Optimizada:")
cm = confusion_matrix(y_test_B, y_pred_B_1)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=['No Diabetes', 'Diabetes'],
            yticklabels=['No Diabetes', 'Diabetes'])
plt.title('Matriz de Confusi칩n - XGBoost')  # Nota: aqu칤 deber칤as cambiar "XGBoost" por "Gradient Boosting"
plt.xlabel('Predicho')
plt.ylabel('Real')
plt.show()

# Mostramos m칠tricas m치s detalladas (precision, recall, f1-score)
print("\nReporte de Clasificaci칩n Mejorado:")
print(classification_report(y_test_B, y_pred_B_1))


# Importamos XGBoost y herramientas de evaluaci칩n
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Configuramos el clasificador XGBoost con hiperpar치metros ajustados
model = xgb.XGBClassifier(
    n_estimators=200,  # N칰mero de 치rboles
    learning_rate=0.1,  # Tasa de aprendizaje (m치s bajo = m치s estable)
    max_depth=4,  # Profundidad m치xima de los 치rboles
    subsample=0.8,  # Fracci칩n de datos usada en cada iteraci칩n (reducci칩n de overfitting)
    colsample_bytree=0.8,  # Fracci칩n de features consideradas por 치rbol
    reg_alpha=0.1,  # Regularizaci칩n L1 (reduce complejidad del modelo)
    reg_lambda=0.1,  # Regularizaci칩n L2 (evita overfitting)
    
    # Peso para manejar desbalance de clases (clase 0 vs clase 1)
    scale_pos_weight=len(y_train_B[y_train_B==0]) / len(y_train_B[y_train_B==1]),

    random_state=38,  # Semilla para reproducibilidad
    eval_metric='logloss'  # M칠trica de evaluaci칩n durante el entrenamiento
)

# Entrenamos el modelo, mostrando el progreso con verbose
model.fit(
    X_train_B, 
    y_train_B,
    eval_set=[(X_test_B, y_test_B)],  # Conjunto de validaci칩n para monitorear performance
    verbose=10  # Muestra el log de cada 10 치rboles
)

# Hacemos predicciones sobre el conjunto de prueba
y_pred_B_2 = model.predict(X_test_B)

# Mostramos el reporte de clasificaci칩n
print("游늵 Reporte de Clasificaci칩n:")
print(classification_report(y_test_B, y_pred_B_2))

# Calculamos y graficamos la matriz de confusi칩n
cm = confusion_matrix(y_test_B, y_pred_B_2)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=['No Diabetes', 'Diabetes'],
            yticklabels=['No Diabetes', 'Diabetes'])
plt.title('Matriz de Confusi칩n - XGBoost')
plt.xlabel('Predicho')
plt.ylabel('Real')
plt.show()
