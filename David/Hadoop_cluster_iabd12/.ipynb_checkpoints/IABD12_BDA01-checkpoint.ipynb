{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tarea Online de Big Data Aplicado\n",
        "\n",
        "## Nombre del alumno:\n",
        "Completa tu nombre aquí.\n"
      ],
      "metadata": {
        "id": "OX_2RmmJwowf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###INTRODUCCIÓN.###\n",
        "\n",
        "El objetivo de esta tarea es desplegar un entorno Hadoop utilizando Docker y realizar un análisis de texto con Python y herramientas de NLP. Asegúrate de seguir los pasos y capturar evidencia de cada etapa.\n",
        "Para ello debes realizar paso a paso el proceso que se explicó en el aula para el despliegue de un cluster Hasoop en Docker. Después podrás entregar esta plantilla sustituyendo las celdas de markdown o de código por lo que se pide. Lee detenidamente el enunciado ya que a veces se pedirá que incluyas una imagen, otras texto y en algunas código ejecutable. El libro de Jupyter que entregues deberá tener las celdas completamente ejecutadas en orden secuencial, no debe tener errores y corresponderá con la ejecución real que obtuviste al ejecutarlo en el contenedor `NameNode` de tu equipo."
      ],
      "metadata": {
        "id": "BQ_aT2SCwreO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comienza copiando este libro NoteBook de Jupyter, `Plantilla_Tarea1_Evaluable_RA1_BDA.ipynb`, en el servidor de Jupyter.Cambia el nombre por `IABDXX_BDA01.ipynb`, donde IABDXX representa tu logon de usuario. Recuerda que tendrás que tener en tu equipo anfitrión, un directorio donde guardar en el futuro tus NoteBooks, este directorio estará montado en un directorio, de tu elección del contenedor que haga de NameNode, de tu cluster Hadoop en Docker. Cualquier cosa que sitúes en este directorio (o en un subdirectorio) será accesible tanto por el anfitrión como por el contenedor.\n",
        "\n",
        "Es posible que al intentar copiar el libro NoteBook de Jupyter `IABDXX_BDA01.ipynb` tengas un problema de permisos. Si esto sucede es porque el contenedor que hace de NameNode en tu cluster de Hadoop en Docker está asignando la propiedad de los archivos al usuario `hdadmin`. Este problema se puede solucionar de distintas formas:\n",
        "\n",
        "* Asignando permisos de escritura a todos los usuarios (comando `chmod` en Linux).\n",
        "* Usando el comando `sudo`.\n",
        "* Copiando el archivo desde el anfitrión al contenedor con el comando `cp` de `docker`:\n",
        "```bash\n",
        "docker cp IABDXX_BDA01.ipynb namenode:/media/notebooks/unidad\\ 1/\n",
        "```\n",
        "* Si todo lo anterior no funciona, pregunta en el foro.\n",
        "\n",
        "Una vez tengas la plantilla de la tarea en el servidor de Jupyter, comienza a completar las celdas."
      ],
      "metadata": {
        "id": "iIjZE1XAyvL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Verifica las versiones de `docker` y `docker-compose`\n",
        "\n",
        "Ejecuta los siguientes comandos en tu equipo anfitrión y añade, al menos, una captura de pantalla:\n",
        "\n",
        "```bash\n",
        "docker --version\n",
        "docker-compose --version\n"
      ],
      "metadata": {
        "id": "mvB_U8Dqw2rX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4HgPDH4w5ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Comienza a crear tu entorno Hadoop + Jupyter, cluster Hadoop + Jupyter, utilizando tus\n",
        "\n",
        "---\n",
        "\n",
        "archivos Dockerfile y docker-compose.yml.\n",
        "\n",
        "Documenta en este paso todo lo necesario para emplementar tu cluster de Hadoop + Jupyter y poder desplegarlo. Si es necesario crea celdas de texto para ir añadiendo contenido.\n"
      ],
      "metadata": {
        "id": "JMOnR7vsNNPT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjmr7slVOHoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3. Despliega el entorno de Hadoop + Jupyter\n",
        "\n",
        "Con tu entorno de Hadoop+ Jupyter ya creado. Usa `docker-compose` para levantar el clúster de Hadoop + Jupyter.\n",
        "Incluye al menos,  una captura de pantalla del comando `docker-compose up` y verifica que el entorno esté funcionando correctamente.\n"
      ],
      "metadata": {
        "id": "7dglrCrfw9RR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPyUwJRcxBoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Muestra el contenido del directorio en Jupyter\n",
        "\n",
        "Ejecuta el siguiente comando para listar el contenido de tu directorio de trabajo:\n",
        "```bash\n",
        "! ls -l\n"
      ],
      "metadata": {
        "id": "pNyeLRj4xB8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FEx97F4RxMXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Conteo de palabras en un archivo de texto\n",
        "\n",
        "Realiza el conteo de palabras de un archivo de texto, por ejemplo, El Quijote, y muestra el resultado.\n",
        "Utiliza Python y comandos de terminal. Para cada \"instrucción\" o código que utilices, necesito que me expliques claramente que es lo que hace.\n",
        "\n",
        "####Aquí te dejo unos ejemplos de comandos y scripts:####\n",
        "\n",
        "```bash\n",
        "! head -30 El_Quijote.txt\n",
        "\n",
        "! tail -370 El_Quijote.txt | head -15\n",
        "\n",
        "with open('El_Quijote') as f:\n",
        "    lines = f.readlines()\n",
        "len(lines)\n",
        "\n",
        "head = 24\n",
        "tail = 360\n",
        "book = lines [head:-tail]\n",
        "\n",
        "book = \"\".join(book)\n",
        "book[0:100]\n",
        "\n",
        "import re\n",
        "book =re.split('\\W+', book)\n",
        "book[0:10]\n",
        "\n",
        "list(filter(lambda word:len(word) == 0, book))\n",
        "\n",
        "book = list(filter(lambda word:len(word), book))\n",
        "book[0:10]\n",
        "\n",
        "\"El  quijote contiene {} palabras\".format(len(book))\n"
      ],
      "metadata": {
        "id": "Gt4RHelAxMqY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBkaeWTxxReA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Palabras más frecuentes en el archivo\n",
        "\n",
        "Crea un proceso en Python que muestre las 10 palabras más comunes en el archivo junto con su frecuencia de aparición.\n",
        "\n",
        "Celda de código de ejemplo:\n",
        "```bash\n",
        "from collections import Counter\n",
        "\n",
        "with open('archivo.txt') as f:\n",
        "    words = f.read().split()\n",
        "\n",
        "word_counts = Counter(words)\n",
        "top_10 = word_counts.most_common(10)\n",
        "print(top_10)\n"
      ],
      "metadata": {
        "id": "0Vpg82j9xRxw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0l6pcWFWxV9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.-(OPCIONAL) Aplicación de Procesamiento de Lenguaje Natural (NLP) en Big Data\n",
        "Los dos ejercicios anteriores son tareas comunes en el ámbito del Big Data y pertenecen a una disciplina conocida como Procesamiento de Lenguaje Natural (NLP). Sin embargo, las soluciones propuestas son simples y presentan varias limitaciones para su uso en un entorno profesional:\n",
        "\n",
        "* Limitación en el manejo de grandes volúmenes de datos: Las soluciones básicas no son adecuadas para procesar grandes cantidades de información. Este desafío es uno de los aspectos fundamentales que abordaremos en las próximas prácticas, donde exploraremos cómo escalar estas soluciones.\n",
        "* Suposiciones poco realistas en la segmentación y análisis de palabras: En el ejercicio 5, se asumió que cualquier carácter no alfanumérico podía separar palabras, lo cual no siempre es cierto. Por ejemplo, ¿cómo se debe tratar \"O.T.A.N.\"? ¿Es una palabra o cinco? Además, en el ejercicio 6, la frecuencia de aparición de palabras no consideró las variaciones de las mismas. Por ejemplo, ¿\"la\" y \"las\" se consideran la misma palabra? ¿Y \"fue\" e \"irá\"? La respuesta a estas preguntas depende del objetivo del análisis y no es uniforme.\n",
        "\n",
        "Las herramientas de NLP, como spaCy, ofrecen funcionalidades avanzadas para resolver estas limitaciones, incluyendo la tokenización (división del texto en palabras) y la [lematización (obtener la forma base de una palabra)](https://es.wikipedia.org/wiki/Lema_(ling%C3%BC%C3%ADstica)#:~:text=El%20lema%20es%20la%20unidad,o%20no%2C%20de%20un%20lema.). Para este ejercicio opcional, intenta resolver los ejercicios 5 y 6 utilizando [spaCy](https://spacy.io/), una biblioteca popular de NLP en Python."
      ],
      "metadata": {
        "id": "sD7eZnl3MkVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utiliza `spaCy` para resolver los ejercicios 5 y 6. Sustituye esta celda por la solución.\n",
        "Usa `spaCy` para realizar un análisis más avanzado de las palabras, incluyendo lematización.\n",
        "Instala el modelo de idioma en español si aún no lo has hecho:\n",
        "```bash\n",
        "! pip install spacy\n",
        "! python3 -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "C2bmX7R9RnF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UyfJrm9AxWN4"
      }
    }
  ]
}