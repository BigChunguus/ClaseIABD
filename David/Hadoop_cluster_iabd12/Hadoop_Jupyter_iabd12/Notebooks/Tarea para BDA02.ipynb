{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plantilla para la Tarea online BDA02\n",
    "\n",
    "# Nombre del alumno: \n",
    "\n",
    "En esta tarea deberás completar las celdas que están incompletas. Se muestra el resultado esperado de la ejecución. Se trata de que implementes un proceso MapReduce que produzca ese resultado. Puedes implementar el proceso MapReduce con el lenguaje y librería que prefieras (`Bash`, Python, `mrjob` ...). Los datos de entrada del proceso son meros ejemplos y el proceso que implementes debería funcionar con esos y cualquier otro fichero de entrada que tenga la misma estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Partiendo del fichero de `notas.txt`, calcula la nota más alta obtenida por cada alumno con un proceso MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, que si tenemos el fichero de notas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notas.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile notas.txt\n",
    "pedro 6 7\n",
    "luis 0 4\n",
    "ana 7\n",
    "pedro 8 1 3\n",
    "ana 5 6 7\n",
    "ana 10\n",
    "luis 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera obtener el siguiente resultado:\n",
    "\n",
    "![solución 1](./img/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "while read line; do\n",
    "    \n",
    "    # Extraemos el nombre de la línea\n",
    "    name=${line%% *}\n",
    "    \n",
    "    # Procesamos nota a nota\n",
    "    for mark in ${line#* }; do\n",
    "                  \n",
    "        # para cada nota emitimos nombre,nota\n",
    "        echo -e \"$name,$mark\"\n",
    "    done    \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x mapper.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana,10\r\n",
      "ana,5\r\n",
      "ana,6\r\n",
      "ana,7\r\n",
      "ana,7\r\n",
      "luis,0\r\n",
      "luis,3\r\n",
      "luis,4\r\n",
      "pedro,1\r\n",
      "pedro,3\r\n",
      "pedro,6\r\n",
      "pedro,7\r\n",
      "pedro,8\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.sh |sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.sh\n",
    "#!/bin/bash\n",
    "prev_name=\n",
    "acc=0\n",
    "declare -i n1\n",
    "declare -i n2\n",
    "n1=0\n",
    "n2=0\n",
    "n_marks=0\n",
    "# Leemos línea a línea\n",
    "while read line; do\n",
    "    \n",
    "    # Extraemos el nombre y la nota\n",
    "    name=${line%,*}\n",
    "    mark=${line#*,} \n",
    "    \n",
    "    if [ -z \"$prev_name\" -o \"$prev_name\" == \"$name\" ]; then\n",
    "           n2=$mark         \n",
    "           if [ \"$n2\" -ge \"$n1\" ]; then\n",
    "               n1=n2\n",
    "               n_marks=$n2\n",
    "           else\n",
    "               n_marks=$n1\n",
    "           fi   \n",
    "    # Cuando el nombre sea diferente, emitimos el nombre anterior,la nota más alta anterior\n",
    "    else\n",
    "        echo $prev_name,$n_marks\n",
    "        n1=0\n",
    "        n2=0\n",
    "    fi\n",
    "    prev_name=$name\n",
    "done\n",
    "           \n",
    "# Emitimos el nombre y la nota más alta del último nombre\n",
    "echo $prev_name,$n_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x reducer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana,10\r\n",
      "luis,4\r\n",
      "pedro,8\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.sh | sort | ./reducer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -rm -f -r /user/root/notas.txt /user/root/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal notas.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   3 root supergroup    2205995 2023-01-18 10:19 /user/root/2000-0.txt\r\n",
      "-rw-r--r--   3 root supergroup         61 2023-01-18 10:37 /user/root/notas.txt\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-01-18 10:19 /user/root/tmp\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedro 6 7\r\n",
      "luis 0 4\r\n",
      "ana 7\r\n",
      "pedro 8 1 3\r\n",
      "ana 5 6 7\r\n",
      "ana 10\r\n",
      "luis 3\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/notas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar] /tmp/streamjob2858468526024176086.jar tmpDir=null\n",
      "2023-01-18 10:37:47,965 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "2023-01-18 10:37:48,219 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "2023-01-18 10:37:48,451 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0010\n",
      "2023-01-18 10:37:48,867 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-01-18 10:37:48,978 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-01-18 10:37:49,072 INFO Configuration.deprecation: mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2023-01-18 10:37:49,219 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674032573936_0010\n",
      "2023-01-18 10:37:49,219 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-18 10:37:49,415 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-18 10:37:49,416 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-18 10:37:49,500 INFO impl.YarnClientImpl: Submitted application application_1674032573936_0010\n",
      "2023-01-18 10:37:49,544 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0010/\n",
      "2023-01-18 10:37:49,547 INFO mapreduce.Job: Running job: job_1674032573936_0010\n",
      "2023-01-18 10:37:56,712 INFO mapreduce.Job: Job job_1674032573936_0010 running in uber mode : false\n",
      "2023-01-18 10:37:56,714 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-18 10:38:05,944 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-01-18 10:38:06,950 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-01-18 10:38:11,984 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-18 10:38:13,010 INFO mapreduce.Job: Job job_1674032573936_0010 completed successfully\n",
      "2023-01-18 10:38:13,125 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=137\n",
      "\t\tFILE: Number of bytes written=832545\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes written=25\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14419\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2531\n",
      "\t\tTotal time spent by all map tasks (ms)=14419\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2531\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14419\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2531\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14765056\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2591744\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=13\n",
      "\t\tMap output bytes=105\n",
      "\t\tMap output materialized bytes=143\n",
      "\t\tInput split bytes=184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=143\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=26\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=465\n",
      "\t\tCPU time spent (ms)=6090\n",
      "\t\tPhysical memory (bytes) snapshot=847720448\n",
      "\t\tVirtual memory (bytes) snapshot=7624359936\n",
      "\t\tTotal committed heap usage (bytes)=656408576\n",
      "\t\tPeak Map Physical memory (bytes)=328364032\n",
      "\t\tPeak Map Virtual memory (bytes)=2540470272\n",
      "\t\tPeak Reduce Physical memory (bytes)=193327104\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2544418816\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25\n",
      "2023-01-18 10:38:13,125 INFO streaming.StreamJob: Output directory: /user/root/output\n"
     ]
    }
   ],
   "source": [
    "! mapred streaming \\\n",
    "    -D mapred.textoutputformat.separator=\",\" \\\n",
    "    -files /media/notebooks/mapper.sh,/media/notebooks/reducer.sh \\\n",
    "    -input /user/root/notas.txt \\\n",
    "    -output /user/root/output \\\n",
    "    -mapper mapper.sh \\\n",
    "    -reducer reducer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2023-01-18 10:38 /user/root/output/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         25 2023-01-18 10:38 /user/root/output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana,10,\r\n",
      "luis,4,\r\n",
      "pedro,8,\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Usando un proceso MapReduce muestra las 10 palabras más utilizadas en `El Quijote`.\n",
    "\n",
    "Lo primero será descargar El Quijote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-18 10:38:20--  https://www.gutenberg.org/files/2000/2000-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... ::ffff:152.19.134.47, 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|::ffff:152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2226045 (2.1M) [text/plain]\n",
      "Saving to: ‘2000-0.txt’\n",
      "\n",
      "2000-0.txt          100%[===================>]   2.12M  2.68MB/s    in 0.8s    \n",
      "\n",
      "2023-01-18 10:38:21 (2.68 MB/s) - ‘2000-0.txt’ saved [2226045/2226045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O '2000-0.txt' https://www.gutenberg.org/files/2000/2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que hicimos en la primera práctica, eliminamos aquellas líneas que son metadata y no forman parte de la obra. Sobrescribimos el fichero sin esas líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2000-0.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "head = 24\n",
    "tail = 360\n",
    "book = lines[head:-tail]\n",
    "\n",
    "with open('2000-0.txt', 'w') as f:\n",
    "    for line in book:\n",
    "        f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado debería ser el mismo que el que obtuvimos en la primera práctica.\n",
    "\n",
    "![solución 2](./img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frecuencyMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile frecuencyMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRMustUsedWord(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_frecuency)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "        \n",
    "    def reducer_frecuency(self, _, counts):\n",
    "        sorted_freqs = sorted(counts, key=lambda t: t[0], reverse=True)\n",
    "        yield None, list(sorted_freqs[0:10])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMustUsedWord.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x frecuencyMR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/frecuencyMR.root.20230118.093823.327141\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/frecuencyMR.root.20230118.093823.327141/output\n",
      "Streaming final output from /tmp/frecuencyMR.root.20230118.093823.327141/output...\n",
      "null\t[[20769, \"que\"], [18408, \"de\"], [18266, \"y\"], [10491, \"la\"], [9874, \"a\"], [8281, \"en\"], [8265, \"el\"], [6334, \"no\"], [4769, \"los\"], [4752, \"se\"]]\n",
      "Removing temp directory /tmp/frecuencyMR.root.20230118.093823.327141...\n"
     ]
    }
   ],
   "source": [
    "! python3 frecuencyMR.py 2000-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/notas.txt\r\n",
      "Deleted /user/root/output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -f -r /user/root/notas.txt /user/root/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/2000-0.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal 2000-0.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/frecuencyMR.root.20230118.093834.974091\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/frecuencyMR.root.20230118.093834.974091/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/frecuencyMR.root.20230118.093834.974091/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6923984540220155432/] [] /tmp/streamjob5478148704942588121.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0011\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1674032573936_0011\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1674032573936_0011\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0011/\n",
      "  Running job: job_1674032573936_0011\n",
      "  Job job_1674032573936_0011 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1674032573936_0011 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/frecuencyMR.root.20230118.093834.974091/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2210091\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=514243\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=502297\n",
      "\t\tFILE: Number of bytes written=1840174\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2210277\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=514243\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16184320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4042752\n",
      "\t\tTotal time spent by all map tasks (ms)=15805\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15805\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3948\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3948\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15805\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3948\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5390\n",
      "\t\tCombine input records=383724\n",
      "\t\tCombine output records=31485\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=271\n",
      "\t\tInput split bytes=186\n",
      "\t\tMap input records=75356\n",
      "\t\tMap output bytes=3779394\n",
      "\t\tMap output materialized bytes=502303\n",
      "\t\tMap output records=383724\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=291958784\n",
      "\t\tPeak Map Virtual memory (bytes)=2540257280\n",
      "\t\tPeak Reduce Physical memory (bytes)=239542272\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2545160192\n",
      "\t\tPhysical memory (bytes) snapshot=820682752\n",
      "\t\tReduce input groups=23139\n",
      "\t\tReduce input records=31485\n",
      "\t\tReduce output records=23139\n",
      "\t\tReduce shuffle bytes=502303\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=62970\n",
      "\t\tTotal committed heap usage (bytes)=637009920\n",
      "\t\tVirtual memory (bytes) snapshot=7624949760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar1065675620770869289/] [] /tmp/streamjob3322615878501967905.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0012\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1674032573936_0012\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1674032573936_0012\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0012/\n",
      "  Running job: job_1674032573936_0012\n",
      "  Job job_1674032573936_0012 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1674032573936_0012 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/frecuencyMR.root.20230118.093834.974091/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=518339\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=150\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=560527\n",
      "\t\tFILE: Number of bytes written=1955254\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=518659\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=150\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7330816\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3002368\n",
      "\t\tTotal time spent by all map tasks (ms)=7159\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7159\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2932\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2932\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7159\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2932\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2760\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=298\n",
      "\t\tInput split bytes=320\n",
      "\t\tMap input records=23139\n",
      "\t\tMap output bytes=514243\n",
      "\t\tMap output materialized bytes=560533\n",
      "\t\tMap output records=23139\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=288886784\n",
      "\t\tPeak Map Virtual memory (bytes)=2541694976\n",
      "\t\tPeak Reduce Physical memory (bytes)=227012608\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2544099328\n",
      "\t\tPhysical memory (bytes) snapshot=802725888\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=23139\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=560533\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=46278\n",
      "\t\tTotal committed heap usage (bytes)=629145600\n",
      "\t\tVirtual memory (bytes) snapshot=7625699328\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/frecuencyMR.root.20230118.093834.974091/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/frecuencyMR.root.20230118.093834.974091/output...\n",
      "null\t[[20769, \"que\"], [18408, \"de\"], [18266, \"y\"], [10491, \"la\"], [9874, \"a\"], [8281, \"en\"], [8265, \"el\"], [6334, \"no\"], [4769, \"los\"], [4752, \"se\"]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/frecuencyMR.root.20230118.093834.974091...\n",
      "Removing temp directory /tmp/frecuencyMR.root.20230118.093834.974091...\n"
     ]
    }
   ],
   "source": [
    "! python3 frecuencyMR.py -r hadoop hdfs:///user/root/2000-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Muestra la clasificación de temporada 2021/2022 de La Liga pero únicamente de los puntos obtenidos como visitante.\n",
    "\n",
    "En [esta Web](https://resultados.as.com/resultados/futbol/primera/2021_2022/clasificacion/) puedes consultar cuántos puntos obtuvo cada equipo fuera de casa.\n",
    "\n",
    "Empezamos descargando el fichero de resultados de la temporada 2021/2022 y renombrándolo a `laliga2122.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-18 10:39:47--  https://www.football-data.co.uk/mmz4281/2122/SP1.csv\n",
      "Resolving www.football-data.co.uk (www.football-data.co.uk)... ::ffff:217.160.0.246, 217.160.0.246\n",
      "Connecting to www.football-data.co.uk (www.football-data.co.uk)|::ffff:217.160.0.246|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172174 (168K) [text/csv]\n",
      "Saving to: ‘laliga2122.csv’\n",
      "\n",
      "laliga2122.csv      100%[===================>] 168.14K  1.07MB/s    in 0.2s    \n",
      "\n",
      "2023-01-18 10:39:47 (1.07 MB/s) - ‘laliga2122.csv’ saved [172174/172174]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O laliga2122.csv https://www.football-data.co.uk/mmz4281/2122/SP1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se espera este resultado:\n",
    "\n",
    "![solución 3](./img/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting laligaMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "    \n",
    "class LaLigaMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, _, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if away_team == \"AwayTeam\":\n",
    "            return\n",
    "        \n",
    "        if result == 'D':            \n",
    "            yield away_team, 1\n",
    "        if result == 'A':            \n",
    "            yield away_team, 3\n",
    "  \n",
    "\n",
    "            \n",
    "    def combiner_points(self, team, points):\n",
    "        yield team, sum(points)\n",
    "            \n",
    "    def reducer_points(self, team, points):\n",
    "        yield None, (team, sum(points))\n",
    "        \n",
    "    def reducer_classification(self, _, points):\n",
    "        yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points,\n",
    "                   combiner=self.combiner_points,\n",
    "                   reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaMR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/laligaMR.root.20230118.093948.807206\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/laligaMR.root.20230118.093948.807206/output\n",
      "Streaming final output from /tmp/laligaMR.root.20230118.093948.807206/output...\n",
      "null\t[[\"Real Madrid\", 42], [\"Barcelona\", 35], [\"Betis\", 33], [\"Ath Madrid\", 30], [\"Sevilla\", 28], [\"Sociedad\", 27], [\"Osasuna\", 25], [\"Villarreal\", 23], [\"Valencia\", 22], [\"Ath Bilbao\", 21], [\"Celta\", 21], [\"Cadiz\", 21], [\"Granada\", 16], [\"Elche\", 15], [\"Vallecano\", 13], [\"Levante\", 13], [\"Mallorca\", 12], [\"Getafe\", 11], [\"Espanol\", 9], [\"Alaves\", 6]]\n",
      "Removing temp directory /tmp/laligaMR.root.20230118.093948.807206...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos ejecución en el cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laligaMR.root.20230118.093949.862851\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laligaMR.root.20230118.093949.862851/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laligaMR.root.20230118.093949.862851/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar5400582868510656920/] [] /tmp/streamjob5953193806913837748.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0013\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1674032573936_0013\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1674032573936_0013\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0013/\n",
      "  Running job: job_1674032573936_0013\n",
      "  Job job_1674032573936_0013 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1674032573936_0013 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMR.root.20230118.093949.862851/step-output/0000\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=428\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=608\n",
      "\t\tFILE: Number of bytes written=836835\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176570\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=428\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7240704\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2928640\n",
      "\t\tTotal time spent by all map tasks (ms)=7071\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7071\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2860\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2860\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7071\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2860\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2120\n",
      "\t\tCombine input records=215\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=243\n",
      "\t\tInput split bytes=300\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=2730\n",
      "\t\tMap output materialized bytes=614\n",
      "\t\tMap output records=215\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=327405568\n",
      "\t\tPeak Map Virtual memory (bytes)=2541330432\n",
      "\t\tPeak Reduce Physical memory (bytes)=196362240\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2543464448\n",
      "\t\tPhysical memory (bytes) snapshot=810942464\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=614\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=647495680\n",
      "\t\tVirtual memory (bytes) snapshot=7624687616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3942019944953876823/] [] /tmp/streamjob7481243328934664552.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0014\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1674032573936_0014\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1674032573936_0014\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0014/\n",
      "  Running job: job_1674032573936_0014\n",
      "  Job job_1674032573936_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1674032573936_0014 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMR.root.20230118.093949.862851/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=642\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=354\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=474\n",
      "\t\tFILE: Number of bytes written=835025\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=956\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=354\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7918592\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4535296\n",
      "\t\tTotal time spent by all map tasks (ms)=7733\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7733\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4429\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4429\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7733\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4429\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2530\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=395\n",
      "\t\tInput split bytes=314\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=428\n",
      "\t\tMap output materialized bytes=480\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=334815232\n",
      "\t\tPeak Map Virtual memory (bytes)=2540920832\n",
      "\t\tPeak Reduce Physical memory (bytes)=224763904\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2543603712\n",
      "\t\tPhysical memory (bytes) snapshot=885268480\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=480\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=681050112\n",
      "\t\tVirtual memory (bytes) snapshot=7624908800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laligaMR.root.20230118.093949.862851/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laligaMR.root.20230118.093949.862851/output...\n",
      "null\t[[\"Real Madrid\", 42], [\"Barcelona\", 35], [\"Betis\", 33], [\"Ath Madrid\", 30], [\"Sevilla\", 28], [\"Sociedad\", 27], [\"Osasuna\", 25], [\"Villarreal\", 23], [\"Valencia\", 22], [\"Celta\", 21], [\"Cadiz\", 21], [\"Ath Bilbao\", 21], [\"Granada\", 16], [\"Elche\", 15], [\"Vallecano\", 13], [\"Levante\", 13], [\"Mallorca\", 12], [\"Getafe\", 11], [\"Espanol\", 9], [\"Alaves\", 6]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laligaMR.root.20230118.093949.862851...\n",
      "Removing temp directory /tmp/laligaMR.root.20230118.093949.862851...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Muestra la diferencia de goles entre el equipo que más goles ha marcado y el que menos goles ha marcado en la temporada 2021/2022 de La Liga.\n",
    "\n",
    "Se espera que el proceso MapReuce produzca una salida similar a la siguiente:\n",
    "\n",
    "![solución 4](./img/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting laligaDMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaDMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "    \n",
    "class LaLigaDMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_goals(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, _, _, home_team, away_team, home_goals, away_goals, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "\n",
    "        if result == 'D': \n",
    "            yield home_team, (int(home_goals))\n",
    "            yield away_team,(int(away_goals))\n",
    "        elif result == 'H':\n",
    "            yield home_team, (int(home_goals))\n",
    "            yield away_team,(int(away_goals))\n",
    "        else:            \n",
    "            yield away_team, (int(away_goals))\n",
    "            yield home_team,(int(home_goals))\n",
    "\n",
    "            \n",
    "    def combiner_goals(self, team, goals):\n",
    "        yield team, sum(goals)\n",
    "            \n",
    "    def reducer_goals(self, team, goals):\n",
    "        yield None, (team, sum(goals))\n",
    "        \n",
    "    def reducer_goals_classification(self,goals, teams):\n",
    "        goals= sorted(teams, key=lambda t: t[1], reverse=True)\n",
    "        goalsFirst = goals[0][0]\n",
    "        goalsLast = goals[-1][0]\n",
    "        goalsDiff=goals[0][1]-goals[-1][1]\n",
    "        \n",
    "        str_one = goalsFirst + \" vs \"+ goalsLast\n",
    "        str_two = \"diferencia de goles \"+ str(goalsDiff)\n",
    "        \n",
    "        yield  (str_one , str_two)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_goals,\n",
    "                   combiner=self.combiner_goals,\n",
    "                   reducer=self.reducer_goals),\n",
    "            MRStep(reducer=self.reducer_goals_classification)\n",
    "        ]\n",
    "\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaDMR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/laligaDMR.root.20230118.094057.042696\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/laligaDMR.root.20230118.094057.042696/output\n",
      "Streaming final output from /tmp/laligaDMR.root.20230118.094057.042696/output...\n",
      "\"Real Madrid vs Alaves\"\t\"diferencia de goles 49\"\n",
      "Removing temp directory /tmp/laligaDMR.root.20230118.094057.042696...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaDMR.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos ejecución en el clúster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laligaDMR.root.20230118.094058.313439\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laligaDMR.root.20230118.094058.313439/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laligaDMR.root.20230118.094058.313439/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6958501444550979084/] [] /tmp/streamjob8163456734948248855.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0015\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1674032573936_0015\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1674032573936_0015\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0015/\n",
      "  Running job: job_1674032573936_0015\n",
      "  Job job_1674032573936_0015 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1674032573936_0015 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaDMR.root.20230118.094058.313439/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=430\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=626\n",
      "\t\tFILE: Number of bytes written=836919\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176572\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=430\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=9352192\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3376128\n",
      "\t\tTotal time spent by all map tasks (ms)=9133\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9133\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3297\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3297\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9133\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3297\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2400\n",
      "\t\tCombine input records=760\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=316\n",
      "\t\tInput split bytes=302\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=9500\n",
      "\t\tMap output materialized bytes=632\n",
      "\t\tMap output records=760\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=329723904\n",
      "\t\tPeak Map Virtual memory (bytes)=2540482560\n",
      "\t\tPeak Reduce Physical memory (bytes)=229736448\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2548912128\n",
      "\t\tPhysical memory (bytes) snapshot=844709888\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=632\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=654311424\n",
      "\t\tVirtual memory (bytes) snapshot=7629176832\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar460495962142054906/] [] /tmp/streamjob8823514598314958645.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0016\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1674032573936_0016\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1674032573936_0016\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0016/\n",
      "  Running job: job_1674032573936_0016\n",
      "  Job job_1674032573936_0016 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1674032573936_0016 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaDMR.root.20230118.094058.313439/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=476\n",
      "\t\tFILE: Number of bytes written=835071\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=961\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=49\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6405120\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2740224\n",
      "\t\tTotal time spent by all map tasks (ms)=6255\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6255\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2676\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2676\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6255\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2676\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1790\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=244\n",
      "\t\tInput split bytes=316\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=430\n",
      "\t\tMap output materialized bytes=482\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=289800192\n",
      "\t\tPeak Map Virtual memory (bytes)=2541289472\n",
      "\t\tPeak Reduce Physical memory (bytes)=225693696\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2544570368\n",
      "\t\tPhysical memory (bytes) snapshot=797855744\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=482\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=631767040\n",
      "\t\tVirtual memory (bytes) snapshot=7626039296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laligaDMR.root.20230118.094058.313439/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laligaDMR.root.20230118.094058.313439/output...\n",
      "\"Real Madrid vs Alaves\"\t\"diferencia de goles 49\"\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laligaDMR.root.20230118.094058.313439...\n",
      "Removing temp directory /tmp/laligaDMR.root.20230118.094058.313439...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaDMR.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Calcula la racha de los últimos cinco partidos de cada equipo en la clasificación final de La Liga en la temporada 2021/2022.\n",
    "\n",
    "[Observa](https://www.google.com/search?q=clasificacion+liga+2021+2022&oq=clasificacion+liga+2021+2022#sie=lg) que las últimas columnas de la clasificación muestran cuál ha sido el resultado de los últimos 5 partidos de cada equipo.\n",
    "\n",
    "![clasificacion](./img/clasificacion.png)\n",
    "\n",
    "Se trata de que muestres la clasificación final junto con los resultados de los últimos 5 partidos. Este ejercicio es un poco más difícil y laborioso que los otros. Si usas `mrjob` probablemente te sea útil utilizar [ordenación secundaria por valor](https://mrjob.readthedocs.io/en/latest/job.html#secondary-sort), aunque también se puede resolver sin hacer uso de ella.\n",
    "\n",
    "Se espera este resultado:\n",
    "\n",
    "![solución 5](./img/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting laligaLast5MR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaLast5MR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from datetime import datetime\n",
    "    \n",
    "class laligaLast5MR(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, date, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "        \n",
    "        date = datetime.strptime(date, \"%d/%m/%Y\").strftime(\"%Y/%m/%d\")\n",
    "\n",
    "        if result == 'D':            \n",
    "            yield home_team, (date, 1)\n",
    "            yield away_team, (date, 1)\n",
    "        elif result == 'H':\n",
    "            yield home_team, (date, 3)\n",
    "            yield away_team, (date, 0)\n",
    "        else:\n",
    "            yield home_team, (date, 0)\n",
    "            yield away_team, (date, 3)\n",
    "            \n",
    "    def reducer_points(self, team, points):\n",
    "        points = list(points)\n",
    "        points = [p for date, p in points]\n",
    "        five_latest_points = points[-5:]\n",
    "        five_latest_points.reverse()\n",
    "        yield None, (team, sum(points), five_latest_points)\n",
    "    \n",
    "    \n",
    "    def reducer_classification(self, _, points):\n",
    "            yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "            \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points, reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    laligaLast5MR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/laligaLast5MR.root.20230118.094212.200790\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/laligaLast5MR.root.20230118.094212.200790/output\n",
      "Streaming final output from /tmp/laligaLast5MR.root.20230118.094212.200790/output...\n",
      "null\t[[\"Real Madrid\", 86, [1, 1, 3, 0, 3]], [\"Barcelona\", 73, [0, 1, 3, 3, 3]], [\"Ath Madrid\", 71, [3, 1, 3, 3, 0]], [\"Sevilla\", 70, [3, 1, 1, 1, 1]], [\"Betis\", 65, [1, 3, 3, 0, 1]], [\"Sociedad\", 62, [0, 3, 3, 0, 1]], [\"Villarreal\", 59, [3, 0, 3, 1, 0]], [\"Ath Bilbao\", 55, [0, 3, 0, 1, 3]], [\"Valencia\", 48, [3, 1, 0, 1, 1]], [\"Osasuna\", 47, [0, 0, 1, 1, 1]], [\"Celta\", 46, [0, 3, 0, 3, 1]], [\"Elche\", 42, [3, 0, 0, 0, 1]], [\"Espanol\", 42, [1, 1, 0, 1, 0]], [\"Vallecano\", 42, [0, 0, 0, 1, 1]], [\"Cadiz\", 39, [3, 1, 0, 3, 1]], [\"Getafe\", 39, [0, 1, 1, 1, 1]], [\"Mallorca\", 39, [3, 3, 1, 0, 0]], [\"Granada\", 38, [1, 0, 3, 3, 1]], [\"Levante\", 35, [3, 3, 0, 3, 1]], [\"Alaves\", 31, [0, 0, 3, 0, 3]]]\n",
      "Removing temp directory /tmp/laligaLast5MR.root.20230118.094212.200790...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaLast5MR.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprobamos este último ejercicio en el clúster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laligaLast5MR.root.20230118.094213.447892\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laligaLast5MR.root.20230118.094213.447892/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laligaLast5MR.root.20230118.094213.447892/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2785827110056456484/] [] /tmp/streamjob6531962679971207225.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0017\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1674032573936_0017\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1674032573936_0017\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0017/\n",
      "  Running job: job_1674032573936_0017\n",
      "  Job job_1674032573936_0017 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1674032573936_0017 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaLast5MR.root.20230118.094213.447892/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=770\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=23946\n",
      "\t\tFILE: Number of bytes written=884099\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176580\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=770\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6724608\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2488320\n",
      "\t\tTotal time spent by all map tasks (ms)=6567\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6567\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2430\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2430\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6567\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2430\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1670\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=181\n",
      "\t\tInput split bytes=310\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=22420\n",
      "\t\tMap output materialized bytes=23952\n",
      "\t\tMap output records=760\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=281346048\n",
      "\t\tPeak Map Virtual memory (bytes)=2540605440\n",
      "\t\tPeak Reduce Physical memory (bytes)=193847296\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2544193536\n",
      "\t\tPhysical memory (bytes) snapshot=753651712\n",
      "\t\tReduce input groups=760\n",
      "\t\tReduce input records=760\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=23952\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1520\n",
      "\t\tTotal committed heap usage (bytes)=613416960\n",
      "\t\tVirtual memory (bytes) snapshot=7624237056\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar313267798175904022/] [] /tmp/streamjob6648866627619715002.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.5:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1674032573936_0018\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1674032573936_0018\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1674032573936_0018\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1674032573936_0018/\n",
      "  Running job: job_1674032573936_0018\n",
      "  Job job_1674032573936_0018 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1674032573936_0018 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaLast5MR.root.20230118.094213.447892/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1155\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=696\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=836\n",
      "\t\tFILE: Number of bytes written=837591\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1479\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=696\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8222720\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2799616\n",
      "\t\tTotal time spent by all map tasks (ms)=8030\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8030\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2734\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2734\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8030\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2734\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1880\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=352\n",
      "\t\tInput split bytes=324\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=790\n",
      "\t\tMap output materialized bytes=842\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=334680064\n",
      "\t\tPeak Map Virtual memory (bytes)=2540605440\n",
      "\t\tPeak Reduce Physical memory (bytes)=229629952\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2543443968\n",
      "\t\tPhysical memory (bytes) snapshot=845762560\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=842\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=664272896\n",
      "\t\tVirtual memory (bytes) snapshot=7624404992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laligaLast5MR.root.20230118.094213.447892/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laligaLast5MR.root.20230118.094213.447892/output...\n",
      "null\t[[\"Real Madrid\", 86, [1, 1, 3, 0, 3]], [\"Barcelona\", 73, [0, 1, 3, 3, 3]], [\"Ath Madrid\", 71, [3, 1, 3, 3, 0]], [\"Sevilla\", 70, [3, 1, 1, 1, 1]], [\"Betis\", 65, [1, 3, 3, 0, 1]], [\"Sociedad\", 62, [0, 3, 3, 0, 1]], [\"Villarreal\", 59, [3, 0, 3, 1, 0]], [\"Ath Bilbao\", 55, [0, 3, 0, 1, 3]], [\"Valencia\", 48, [3, 1, 0, 1, 1]], [\"Osasuna\", 47, [0, 0, 1, 1, 1]], [\"Celta\", 46, [0, 3, 0, 3, 1]], [\"Elche\", 42, [3, 0, 0, 0, 1]], [\"Espanol\", 42, [1, 1, 0, 1, 0]], [\"Vallecano\", 42, [0, 0, 0, 1, 1]], [\"Cadiz\", 39, [3, 1, 0, 3, 1]], [\"Getafe\", 39, [0, 1, 1, 1, 1]], [\"Mallorca\", 39, [3, 3, 1, 0, 0]], [\"Granada\", 38, [1, 0, 3, 3, 1]], [\"Levante\", 35, [3, 3, 0, 3, 1]], [\"Alaves\", 31, [0, 0, 3, 0, 3]]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laligaLast5MR.root.20230118.094213.447892...\n",
      "Removing temp directory /tmp/laligaLast5MR.root.20230118.094213.447892...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaLast5MR.py -r hadoop laliga2122.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
